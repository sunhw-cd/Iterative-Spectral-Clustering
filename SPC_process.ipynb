{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31085bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard library ---\n",
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "\n",
    "# --- Third-party scientific stack ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# --- Machine learning / statistics ---\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE, SpectralEmbedding\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# --- SciPy ---\n",
    "from scipy.linalg import LinAlgError, qr, svd\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "# --- Utilities ---\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import parallel_backend\n",
    "import numexpr as ne\n",
    "\n",
    "# --- Local / custom modules ---\n",
    "import fpsample\n",
    "\n",
    "\n",
    "\n",
    "# Self-written libraries\n",
    "sys.path.append(join(os.getcwd(), \"library\"))\n",
    "import SPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f0d6e",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f165e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "# Input your own file path here\n",
    "file_path = r\"C:\\used\\data\\Clustering_Analysis\\Clustering_Analysis\\Correlation_Map.bin\"\n",
    "\n",
    "# Use memory mapping to read the large matrix\n",
    "fragment = np.memmap(\n",
    "    file_path,\n",
    "    dtype=np.float64,   # Explicitly specify data type\n",
    "    mode=\"r\",           # Read-only mode\n",
    "    shape=(28800, 28800)\n",
    ")\n",
    "\n",
    "# Time index\n",
    "t_index = np.arange(fragment.shape[0])\n",
    "idx=np.arange(fragment.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09576e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Labels Loading ---\n",
    "results = pd.read_csv(\n",
    "    r'D:\\used\\Log_Data_Coherent_Correlation_Imaging.txt',\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Rename columns properly (removed trailing empty column name)\n",
    "results.columns = [\n",
    "    'Timestamp', 'Helicity', 'State', 'Mode',\n",
    "    'Topography_Nr', 'Translation_y', 'Translation_x', 'FileName',''\n",
    "]\n",
    "\n",
    "# Select the row of interest (example: idx+1)\n",
    "mode_value = results.loc[idx + 1, 'Mode']\n",
    "state_value = results.loc[idx + 1, 'State']\n",
    "\n",
    "# Find indices corresponding to \"Not assigned\"\n",
    "filter_index = np.where(mode_value == ' Not assigned')\n",
    "print(\"Indices with 'Not assigned':\", filter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mode label to integer \n",
    "modes_encoded, unique_items = pd.factorize(mode_value)\n",
    "print(\"Unique items:\", unique_items)\n",
    "#print(\"Encoded modes:\", modes_encoded)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "modes = modes_encoded.astype(float)\n",
    "states = state_value.to_numpy().astype(float)\n",
    "\n",
    "# Replace \"Not assigned\" entries with NaN\n",
    "modes[filter_index]=np.nan\n",
    "states[filter_index]=np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0001c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Relabel function ---\n",
    "def relabel(labels):\n",
    "    \"\"\"\n",
    "    Relabels an array of labels:\n",
    "    1. Replace each unique label with the median index of its occurrences.\n",
    "    2. Reassign labels to consecutive integers starting from 0.\n",
    "    NaN values are preserved.\n",
    "    \"\"\"\n",
    "    labels = np.array(labels, dtype=float)\n",
    "\n",
    "    # Get unique non-NaN items\n",
    "    unique_items = np.unique(labels[~np.isnan(labels)])\n",
    "    print(\"Unique items before relabeling:\", unique_items)\n",
    "\n",
    "    # Step 1: Replace each label with the median index of its occurrences\n",
    "    for item in unique_items:\n",
    "        indices = np.where(labels == item)[0]\n",
    "        median_index = indices[len(indices) // 2]  # middle occurrence\n",
    "        labels[indices] = median_index\n",
    "\n",
    "    # Step 2: Normalize labels to consecutive integers\n",
    "    unique_items = np.sort(np.unique(labels[~np.isnan(labels)]))\n",
    "    for new_label, old_value in enumerate(unique_items):\n",
    "        labels[labels == old_value] = new_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "# Apply relabeling\n",
    "modes = relabel(modes)\n",
    "states = relabel(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 7))\n",
    "plt.plot(modes,label='Modes')\n",
    "plt.plot(states,label='States')\n",
    "plt.legend()\n",
    "plt.title(\"Modes vs States\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37d299",
   "metadata": {},
   "source": [
    "# Clustering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Iterative Spectral Clustering Workflow\n",
    "# ============================================\n",
    "\n",
    "# Initialize cluster dictionary\n",
    "cluster_dict = {\n",
    "    \"Cluster_Nr\": 0,                                # Cluster index number\n",
    "    \"Cluster_Frames\": np.arange(fragment.shape[0]), # All frames initially belong to cluster 0\n",
    "    \"Nr_Frames\": 0,                                 # Number of frames in cluster (updated later)\n",
    "    \"Threshold\": 0,                                 # Reclustering threshold (updated later)\n",
    "    \"Order\": 0,                                     # Order of correlation (0 for first cluster, >0 for subclusters)\n",
    "    \"Metric\": 0,                                    # Similarity metric used (\"correlation\" or \"cosine\")\n",
    "    \"eigenvalues\": np.arange(fragment.shape[0]),    # Placeholder for eigenvalues\n",
    "}\n",
    "\n",
    "# Store clusters in a list\n",
    "cluster = [cluster_dict]\n",
    "\n",
    "# Clustering parameters\n",
    "first_threshold = 0.05   # Threshold for first cluster\n",
    "rc_threshold = 0.15      # Threshold for reclustering subclusters\n",
    "force_iterations = 3     # Force splitting of clusters for how many iterations\n",
    "order = 1                # Correlation order (1 or 2)\n",
    "metric = \"correlation\"   # Similarity metric (\"correlation\" or \"cosine\")\n",
    "\n",
    "# Reset loop variables\n",
    "cluster_idx = 0\n",
    "stop = False\n",
    "\n",
    "# Close all previous plots\n",
    "plt.close(\"all\")\n",
    "\n",
    "# ============================================\n",
    "# Iterative loop over clusters\n",
    "# ============================================\n",
    "while not stop:\n",
    "    print(\"\")\n",
    "    print(f\"=========== Clustering of Cluster Index: {cluster_idx} ===========\")\n",
    "\n",
    "    # Only process clusters with sufficient frames\n",
    "    if len(cluster[cluster_idx][\"Cluster_Frames\"]) > 30:\n",
    "\n",
    "        # --- Sampling ---\n",
    "        if cluster_idx == 0:\n",
    "            # First cluster: sample directly from full fragment space\n",
    "            Sampler_1 = SPC.Sampler(\n",
    "                cluster[cluster_idx][\"Cluster_Frames\"],\n",
    "                feature_space=fragment,\n",
    "                max_samples=500,\n",
    "                min_samples=500\n",
    "            )\n",
    "            train_idx, test_idx, train_SN, test_SN = Sampler_1.shuffle(\n",
    "                proportion=0.5, method=\"random\"\n",
    "            )\n",
    "            print(\"Cluster size:\", len(cluster[cluster_idx][\"Cluster_Frames\"]),\n",
    "                    \"Train size:\", len(train_idx),\n",
    "                    \"Test size:\", len(test_idx))\n",
    "\n",
    "            # Build correlation maps\n",
    "            train_corr = SPC.reconstruct_correlation_map(\n",
    "                train_idx, train_idx, fragment, 0, metric\n",
    "            )\n",
    "            test_corr = SPC.reconstruct_correlation_map(\n",
    "                test_idx, train_idx, fragment, 0, metric\n",
    "            ) if len(test_idx) != 0 else None\n",
    "\n",
    "            threshold = first_threshold\n",
    "\n",
    "        else:\n",
    "            # Subsequent clusters: sample from sub-fragment space\n",
    "            Sampler_2 = SPC.Sampler(\n",
    "                cluster[cluster_idx][\"Cluster_Frames\"],\n",
    "                feature_space=fragment[np.ix_(\n",
    "                    cluster[cluster_idx][\"Cluster_Frames\"],\n",
    "                    cluster[cluster_idx][\"Cluster_Frames\"]\n",
    "                )],\n",
    "                max_samples=500,\n",
    "                min_samples=500\n",
    "            )\n",
    "            train_idx, test_idx, train_SN, test_SN = Sampler_2.shuffle(\n",
    "                proportion=0.2, method=\"random\"\n",
    "            )\n",
    "            print(\"Cluster size:\", len(cluster[cluster_idx][\"Cluster_Frames\"]),\n",
    "                    \"Train size:\", len(train_idx),\n",
    "                    \"Test size:\", len(test_idx))\n",
    "\n",
    "            # Build correlation maps\n",
    "            train_corr = SPC.reconstruct_correlation_map(\n",
    "                train_idx, train_idx, fragment, order, metric\n",
    "            )\n",
    "            test_corr = SPC.reconstruct_correlation_map(\n",
    "                test_idx, train_idx, fragment, order, metric\n",
    "            ) if len(test_idx) != 0 else None\n",
    "\n",
    "            threshold = rc_threshold\n",
    "\n",
    "        # --- Store cluster parameters ---\n",
    "        cluster[cluster_idx][\"Nr_Frames\"] = len(train_idx) + len(test_idx)\n",
    "        cluster[cluster_idx][\"Order\"] = order\n",
    "        cluster[cluster_idx][\"Metric\"] = metric\n",
    "        cluster[cluster_idx][\"Threshold\"] = threshold\n",
    "\n",
    "        # --- Spectral Embedding ---\n",
    "        print(\"Calculating Clustering\")\n",
    "        try:\n",
    "            eigenvalues, eigenvectors, Laplacian_mat, affinity_mat = SPC.Spectral_embeding(\n",
    "                train_corr, sigma_nn=2\n",
    "            )\n",
    "\n",
    "            # Save eigen info\n",
    "            cluster[cluster_idx][\"eigenvalues\"] = eigenvalues\n",
    "            cluster[cluster_idx][\"eigenvectors\"] = eigenvectors\n",
    "            cluster[cluster_idx][\"eigenvec_shape\"] = np.shape(eigenvectors)\n",
    "\n",
    "            print(f\"Smallest Eigenvalue = {eigenvalues[1]:.4f}\")\n",
    "\n",
    "            # --- Reclustering condition ---\n",
    "            print(\"Recluster\")\n",
    "            if eigenvalues[1] < threshold:\n",
    "                start_time = time.time()\n",
    "\n",
    "                if cluster_idx == 0:\n",
    "                    # For first cluster: number of clusters = count of eigenvalues below threshold\n",
    "                    nr_cluster = np.sum(eigenvalues < threshold)\n",
    "                else:\n",
    "                    # For subclusters: use multiple criteria\n",
    "                    nr_max_cluster = np.sum(eigenvalues < threshold)\n",
    "                    nr_min_cluster = np.sum(eigenvalues < threshold / 10)\n",
    "                    nr_gap_cluster = SPC.eigerngap(eigenvalues[:nr_max_cluster + 1])\n",
    "\n",
    "                    #Choose cluster number based on gap/min criteria\n",
    "                    #nr_cluster = np.max([nr_gap_cluster, nr_min_cluster])\n",
    "                    nr_cluster = 2\n",
    "\n",
    "                    print(f\"nr_max_cluster {nr_max_cluster}! Recluster number {nr_cluster}!\")\n",
    "\n",
    "                print(f\"Reclustering condition satisfied \"\n",
    "                      f\"(Eigenvalue = {eigenvalues[nr_cluster - 1]:.4f})! \"\n",
    "                      f\"Recluster number {nr_cluster}\")\n",
    "\n",
    "                # --- Train clustering ---\n",
    "                cluster_space, train_labels, model = SPC.clustering(\n",
    "                    eigenvectors,\n",
    "                    n_components=nr_cluster,\n",
    "                    n_clusters=nr_cluster,\n",
    "                    assign_labels=\"kmeans\"\n",
    "                )\n",
    "\n",
    "                # --- Test clustering ---\n",
    "                if len(test_idx) != 0:\n",
    "                    test_norm_affinity = SPC.local_scaled_affinity(test_corr, train_corr, k=2)\n",
    "                    # Normalize affinity matrix\n",
    "                    test_norm_affinity=test_norm_affinity/np.expand_dims(np.sum(test_norm_affinity,axis=-1),axis=-1)\n",
    "                    test_embed = np.dot(\n",
    "                        test_norm_affinity @ affinity_mat,\n",
    "                        eigenvectors[:, :nr_cluster] / (1 - eigenvalues[:nr_cluster])\n",
    "                    )\n",
    "                    test_labels = model.predict(test_embed)\n",
    "                else:\n",
    "                    test_labels = []\n",
    "\n",
    "                # --- Merge train/test labels ---\n",
    "                labels = np.zeros(len(train_labels) + len(test_labels))\n",
    "                labels[train_SN] = train_labels\n",
    "                labels[test_SN] = test_labels\n",
    "\n",
    "                # Timing\n",
    "                end_time = time.time()\n",
    "                print(f\"Time for embedding: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "                # --- Process sub-clusters ---\n",
    "                cluster = SPC.process_cluster(\n",
    "                    model, cluster, cluster_idx, labels,\n",
    "                    save=True\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                print(f\"Cluster {cluster_idx} does not satisfy reclustering \"\n",
    "                      f\"(smallest eigenvalue: {eigenvalues[1]:.4f})!\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Cluster {cluster_idx} failed due to error: {e}\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"Small cluster size: Skipping!\")\n",
    "\n",
    "    # --- Update index ---\n",
    "    cluster_idx += 1\n",
    "    if cluster_idx >= len(cluster):\n",
    "        stop = True\n",
    "\n",
    "# Final feedback\n",
    "print(\"Iterative clustering algorithm finished!\")\n",
    "\n",
    "# Remove empty clusters\n",
    "cluster = list(filter(None, cluster))\n",
    "\n",
    "print(f\"You determined {len(cluster)} clusters! (Threshold {threshold:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81934206",
   "metadata": {},
   "source": [
    "# Reorder and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f01334",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cluster[12][\"eigenvalues\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c211998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of clusters\n",
    "print(f\"Number of clusters: {len(cluster)}\")\n",
    "\n",
    "# Initialize labels with -1\n",
    "my_labels = np.full(fragment.shape[0], -1, dtype=float)\n",
    "\n",
    "# Assign cluster indices to frames\n",
    "for i, c in enumerate(cluster):\n",
    "    my_labels[c['Cluster_Frames']] = i\n",
    "\n",
    "# Mask out filtered indices\n",
    "my_labels[filter_index] = np.nan\n",
    "\n",
    "# Relabel \n",
    "labels = relabel(my_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering result\n",
    "\n",
    "# Wrap into an xarray DataArray with dimension labels\n",
    "data = xr.DataArray(\n",
    "    labels,\n",
    "    dims=(\"label\"),\n",
    "    coords={\"label\": np.arange(np.shape(labels)[0])},\n",
    "    name=\"my_label\"\n",
    ")\n",
    "\n",
    "\n",
    "# Save to NetCDF (common scientific format)\n",
    "fname=\"Clustering_result.nc\"\n",
    "if os.path.exists(fname):\n",
    "    os.remove(fname)\n",
    "data.to_netcdf(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d980a0d",
   "metadata": {},
   "source": [
    "# Analyse Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_idx = np.arange(0, 28800)\n",
    "compare_xs = [5000, 10000, 15000, 20000, 25000]\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(22, 22), sharex=True)\n",
    "\n",
    "# Modes\n",
    "axs[0].plot(modes[show_idx], label='CCI Modes', alpha=0.6, color='green')\n",
    "axs[0].legend(fontsize=16) \n",
    "axs[0].set_title(\"CCI Modes\", fontsize=22)\n",
    "\n",
    "# My label\n",
    "axs[1].plot(labels[show_idx], label='My label', alpha=0.6, color='blue')\n",
    "axs[1].legend(fontsize=16)\n",
    "axs[1].set_title(\"My label\", fontsize=22)\n",
    "\n",
    "# States\n",
    "axs[2].plot(states[show_idx], label='CCI States', alpha=0.6, color='red')\n",
    "axs[2].legend(fontsize=16)\n",
    "axs[2].set_title(\"CCI States\", fontsize=22)\n",
    "\n",
    "# \n",
    "axs[0].set_ylabel('mode', fontsize=22, labelpad=10) \n",
    "axs[1].set_ylabel('cluster', fontsize=22, labelpad=10) \n",
    "axs[2].set_ylabel('state', fontsize=22, labelpad=10) \n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='both', labelsize=22)         \n",
    "    ax.grid(True)\n",
    "    for x in compare_xs:\n",
    "        ax.axvline(x=x, color='black', linestyle='--', linewidth=1.2)\n",
    "\n",
    "axs[2].set_xlabel('Snapshot', fontsize=22, labelpad=10)\n",
    "fig.suptitle(\"Result Comparison\", fontsize=22)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_assignment(test_assignment, reference_assignment):\n",
    "    test_assignment = np.squeeze(test_assignment)\n",
    "    reference_assignment = np.squeeze(reference_assignment)\n",
    "\n",
    "    # Which clusters do we have?\n",
    "    clusters_test = np.unique(test_assignment[~np.isnan(test_assignment)])\n",
    "    clusters_ref = np.unique(reference_assignment[~np.isnan(reference_assignment)])\n",
    "\n",
    "    sz = np.max(\n",
    "        [\n",
    "            len(clusters_test),\n",
    "            len(clusters_ref),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Split test clusters\n",
    "    test_cluster_matrix = np.zeros(\n",
    "        (sz, sz, len(test_assignment)),\n",
    "        dtype=bool,\n",
    "    )\n",
    "    for i in range(len(clusters_test)):\n",
    "        tmp = test_assignment == i\n",
    "        test_cluster_matrix[i, :] = tmp[np.newaxis, :]\n",
    "\n",
    "    # Split reference clusters\n",
    "    clusters_ref = np.unique(reference_assignment[~np.isnan(reference_assignment)])\n",
    "\n",
    "    ref_cluster_matrix = np.zeros((sz, len(reference_assignment)))\n",
    "    for i in range(len(clusters_ref)):\n",
    "        tmp = reference_assignment == i\n",
    "        ref_cluster_matrix[i, :] = tmp[np.newaxis, :]\n",
    "\n",
    "    # Compare test and reference clusters\n",
    "    comparison = test_cluster_matrix * ref_cluster_matrix[np.newaxis, ...]\n",
    "\n",
    "    # What is the percentage of overlap of each cluster?\n",
    "    spread = np.sum(comparison, axis=-1)\n",
    "    nr_frames = np.sum(test_cluster_matrix, axis=-1)\n",
    "    spread_percentage = spread / nr_frames\n",
    "    spread_percentage[np.isinf(spread_percentage)] = 0\n",
    "\n",
    "    # What is the error_rate?\n",
    "    error = 1 - np.max(spread_percentage, axis=-1)\n",
    "    error_rate = error * (nr_frames[:, 0] / np.sum(nr_frames[:, 0]))\n",
    "    total_error = np.nansum(error_rate)\n",
    "\n",
    "    # To-do: Put into pandas dataframe\n",
    "\n",
    "    # Create some nice plots\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "\n",
    "    mi, ma = np.nanpercentile(spread[spread != 0], [5, 95])\n",
    "    m00 = ax[0, 0].imshow(np.transpose(spread), vmin=mi, vmax=ma)\n",
    "    ax[0, 0].set_ylabel(\"Reference assignment cluster (expanded)\")\n",
    "    ax[0, 0].set_xlabel(\"Test assignment cluster (expanded)\")\n",
    "    ax[0, 0].set_ylim([0, len(clusters_ref)])\n",
    "    # cb00 = plt.colorbar(m00, label=\"Distribution in reference clusters\")\n",
    "\n",
    "    mi, ma = np.nanpercentile(spread_percentage[spread_percentage != 0], [1, 99])\n",
    "    m01 = ax[0, 1].imshow(np.transpose(spread_percentage), vmin=mi, vmax=ma)\n",
    "    ax[0, 1].set_ylabel(\"Reference assignment cluster (expanded)\")\n",
    "    ax[0, 1].set_xlabel(\"Test assignment cluster (expanded)\")\n",
    "    ax[0, 1].set_ylim([0, len(clusters_ref)])\n",
    "    # cb01 = plt.colorbar(m01, label=\"Distribution in reference clusters in %\")\n",
    "\n",
    "    ax[1, 0].plot(error)\n",
    "    ax[1, 0].grid()\n",
    "    ax[1, 0].set_xlabel(\"Test cluster\")\n",
    "    ax[1, 0].set_ylabel(\"Relative error of each cluster\")\n",
    "\n",
    "    ax[1, 1].plot(100 * error_rate)\n",
    "    ax[1, 1].grid()\n",
    "    ax[1, 1].set_xlabel(\"Test cluster\")\n",
    "    ax[1, 1].set_ylabel(\"Cluster Error weighted with total number of frames in %\")\n",
    "\n",
    "    print(\"Your total assignment error: %.2f %%\" % (100 * total_error))\n",
    "\n",
    "    return 100 * total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b13a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_assignment(labels,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db706fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98330f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
